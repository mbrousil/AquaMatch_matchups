---
title: "AquaMatch WQP Matchups"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 80
---

```{r file-setup}
library(tidyverse) 
library(data.table)
# FYI some conflicts with data.table, mostly {dplyr} and {lubridate} 
# functions, to use the tidyverse version of these functions you will 
# need to use the `package_name::function()` (eg: `dplyr::select()`) to
# access
library(sf)
library(sfheaders)
library(arrow)
library(googledrive)
library(tictoc)
library(scales)
library(tigris)

# Files sourced from Drive are available to any Google User, the console will walk through authentication steps.
drive_auth("therossyndicate@gmail.com")
```

# Purpose

This markdown file walks through the process of creating matchups from the
AquaMatch harmonized data from the Water Quality Portal and the AquaMatch Landsat C2
SR data across the United States and its Territories.

## Bring in data

### *In situ* data
We access AquaMatch *in situ* data directly from the EDI Data Repository. We adapt
helper scripts generated by EDI to read in the data - these are stored in the `src`
folder of the matchup repository. **Note**: We've edited the helper scripts to keep
the `harmonized_local_time` column in string format, rather than datetime format,
because each row in the local time column is intended to have its own corresponding
time zone specified in the `harmonized_tz` column. R requires a single time zone for
datetime colums. Also, these scripts are based on specific versions of the published
data products. If revised versions are published then the scripts may need to be
updated in order to grab the newest data versions. The code chunk below defines which
published parameters (Chlorophyll *a*, Dissolved Organic Carbon, Secchi Disk Depth,
Total Suspended Solids, etc.) to download from EDI.

```{r download-edi}
# Where to put the data
edi_download_path <- "in/edi_data"

# Check for download directory, make if it does not exist
if (!dir.exists(edi_download_path)) {
  dir.create(edi_download_path, recursive = TRUE)
}

# This code chunk will take a few minutes to run if not run previously

# Chla
# Citation: 
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, J. De La Torre, and M.R. Ross. 2024. AquaMatch Chlorophyll a Data from Water Quality Portal: ~1970-2024 ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/2f750544112e5408928dd9a61e6ace30 (Accessed 2025-09-17).
source(file = "src/retrieve_chla.R")

# DOC
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, and M.R. Ross. 2024. AquaMatch Dissolved Organic Carbon Data from Water Quality Portal: ~1970-2024 ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/31d271897f074da990a4f25108ff2a40 (Accessed 2025-09-17).
source(file = "src/retrieve_doc.R")

# SDD
# De La Torre, J., B.G. Steele, M.R. Brousil, M.F. Meyer, K. Willi, and M.R. Ross. 2025. AquaMatch Secchi Disk Depth Data from Water Quality Portal: ~1970-2024 ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/542a305e8484ae5abc33881bd7761308 (Accessed 2025-09-17).
source(file = "src/retrieve_sdd.R")

# TSS
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, J.R. Gardner, and M.R. Ross. 2025. AquaMatch Total Suspended Solids from Water Quality Portal ~1970-2025 ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/6cde7f8613ef5e7e2bef3a19160e02ea (Accessed 2025-09-17).
source(file = "src/retrieve_tss.R")

all_in_situ <- bind_rows(wqp_chla, wqp_doc, wqp_sdd, wqp_tss)

write_feather(x = all_in_situ,
              sink = file.path(edi_download_path, "all_in_situ_wqp.feather"))

rm(wqp_chla, wqp_doc, wqp_sdd, wqp_tss)
gc()
```

### Remote sensing data
Next we download the siteSR data. This is currently done from Google Drive, but
in the future this will change once the data are formally published.
```{r download-rs}
rs_download_path <- "in/siteSR_data/"

# Check for download directory, make if it does not exist
if (!dir.exists(rs_download_path)) {
  dir.create(rs_download_path, recursive = TRUE)
}

# At this time, the C2 data are sourced from Drive, will be updated once the
# pub is out.
# This may take several minutes if the files have not yet been downloaded.
# Landsat 4-9 paths are loaded in place of the files themselves due to their size. 
source("src/load_Landsat_C2_SRST.R")

# Confirm that site location data were loaded
dim(site_locs)
```


## Making Matchups

There are many different ways and filters that can be applied to matchups. River
sites change more rapidly than lake sites (generally speaking), so matchup
windows (the time allowed between satellite acquisition and in situ
observation/measurement) should be shorter. We use data.table functions for
these applications because they are the most efficient for datasets of this
size.

The following function makes matchups based on the prescribed window:

```{r define-matchup-function}
#' @param sat_fp: Vector of filepaths to the satellite data
#' @param wqp_data: in situ data 
#' @param site_info Data frame of site metadata containing loc_id and siteSR_id cols
#' @param window: Integer of number of days on either side of the insitu measuerment
make_matchups <- function(sat_fp, wqp_data, site_info, window) {
  
  # Create min and max times within WQP data corresponding to specified window
  wqp_data <- wqp_data %>%
    mutate(
      min_time = ActivityStartDate - days(window),
      max_time = ActivityStartDate + days(window)
    )
  
  # Add siteSR_id to WQP data to allow join with siteSR
  wqp_w_ids <- wqp_data %>%
    left_join(x = .,
              y = select(site_info, loc_id, siteSR_id),
              by = c("MonitoringLocationIdentifier" = "loc_id"))
  
  # Iterate over 
  wqp_matchups <- map(
    .x = sat_fp,
    .f = ~{
      temp_file <- read_feather(file = .x) %>%
        filter(siteSR_id %in% unique(wqp_w_ids$siteSR_id)) %>%
        # This duplicate column will be sacrificed during the join below
        mutate(join_date = date)
      
      # Time range join with data.table
      matchup <- data.table(wqp_w_ids)[data.table(temp_file),
                                       on = .(siteSR_id,
                                              max_time >= join_date,
                                              min_time <= join_date),
                                       nomatch = NULL] %>%
        as_tibble() %>%
        # Calc time difference between reported in situ time and overpass time
        mutate(time_diff = ActivityStartDate - date)
      
      # Clean up
      rm(temp_file)
      gc()
      
      return(matchup)
    }
  ) %>%
    # Stack matchups across all missions 
    bind_rows()
  
  # Return to user with addition of col indicating datetime of (in situ - overpass)
  wqp_matchups
}
```

Perform the matchups:
```{r make-matchups}
# Time the process with {tictoc}
tic()
siteSR_matchups <- make_matchups(
  sat_fp = c(LS4_fp, LS5_fp, LS7_fp, LS8_fp, LS9_fp),
  wqp_data = all_in_situ, 
  site_info = site_locs,
  window = 5
)
toc()

# Remove unnecessary
rm(site_locs)
gc()

```

## Visualize matchups

Next we'll do some visual checks of the resulting data. First we'll clean up the
geospatial data needed for mapping. Most of the records have WGS84 datum, but a
few don't. How many will we miss if we drop the non-WGS84? Should be very few:

```{r check-datum}
siteSR_matchups %>%
  filter(datum != "WGS84") %>%
  nrow()
```


We can move ahead to plotting the geographic record distribution. Here they are for broken down by the `parameter` column within the conterminous U.S.:
```{r make-param-maps}
state_selection <- states(progress_bar = FALSE) %>%
  st_transform(crs = 9311)

matchups_sf <- siteSR_matchups %>%
  select(parameter, lat, lon, datum) %>%
  filter(datum == "WGS84") %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) %>%
  st_transform(crs = 9311)

# Conterminous US sf object
conterminous_us <- tigris::states(progress_bar = FALSE) %>%
  st_transform(crs = 9311) %>%
  filter(!(NAME %in% c("Alaska", "Hawaii", "American Samoa",
                       "Guam", "Puerto Rico",
                       "United States Virgin Islands",
                       "Commonwealth of the Northern Mariana Islands")))

# Other US territories sf object
non_conterminous_us <- tigris::states(progress_bar = FALSE) %>%
  st_transform(crs = 9311) %>%
  filter((NAME %in% c("Alaska", "Hawaii", "American Samoa",
                      "Guam", "Puerto Rico",
                      "United States Virgin Islands",
                      "Commonwealth of the Northern Mariana Islands")))

# Focal records for the map
trim_recs <- matchups_sf[conterminous_us, ] %>%
  sf_to_df(fill = TRUE) 

trim_recs %>%
  ggplot() +
  geom_hex(aes(x = x, y = y),
           binwidth = 100000) +
  geom_sf(data = conterminous_us,
          color = "black",
          fill = NA) +
  # facet_grid(rows = vars(parameter)) +
  facet_wrap(vars(parameter)) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_c("Record count",
                       trans = "log",
                       breaks = breaks_log(n = 6),
                       labels = label_number(big.mark = ",")) +
  ggtitle(
    label = "Matchup counts across the US by parameter",
    subtitle = paste0(
      "Not shown: ",
      comma(nrow(matchups_sf[non_conterminous_us,])),
      " records from outside the conterminous US"
    )
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.key.width = unit(1, "cm")
  )


# Clean up the objects after using them
rm(conterminous_us, non_conterminous_us, trim_recs)
gc()
```

And here is a plot of all records, not limited to the conterminous U.S.:
```{r make-full-map}
matchups_sf %>%
  sf_to_df(fill = TRUE) %>%
  ggplot() +
  geom_hex(aes(x = x, y = y),
           binwidth = 100000) +
  geom_sf(data = state_selection,
          color = "black",
          fill = NA) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_c("Record count",
                       trans = "log",
                       breaks = breaks_log(n = 6),
                       labels = label_number(big.mark = ",")) +
  ggtitle(label = "Matchup counts") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.key.width = unit(1.25, "cm")
  )

rm(matchups_sf, state_selection)
gc()
```

Comparison of data distributions for the in situ data and matchup data:
```{r plot-quantiles}
siteSR_quantile_df <- siteSR_matchups %>%
  select(parameter, harmonized_value, harmonized_units) %>%
  mutate(dataset = "AquaMatch matchups") 

quantile_plot_data <- siteSR_quantile_df %>%
  rbind(
    all_in_situ %>%
      select(parameter, harmonized_value, harmonized_units) %>%
      mutate(dataset = "WQP in situ")
  ) %>%
  mutate(dataset = factor(dataset,
                          levels = c("WQP in situ", "AquaMatch matchups")),
         param_units = factor(
           x = parameter,
           levels = c("chlorophyll", "doc", "sdd", "ssc", "tss"),
           labels = c("Chla (ug/L)",
                      "DOC (mg/L)",
                      "SDD (m)",
                      "SSC (mg/L)",
                      "TSS (mg/L)")
         )
  )

quantile_perc <- c(0,0.05,.25,.5,0.75,.95,1)
quantile_labs <- c("<5%","5-25%","25-50%","50-75%","75-95%",">95%")

quantile_colors <- RColorBrewer::brewer.pal(length(quantile_perc) + 2,"GnBu")[-c(1:2)]

matchup_quantiles <- siteSR_quantile_df %>%
  mutate(
    param_units = factor(
      x = parameter,
      levels = c("chlorophyll", "doc", "sdd", "ssc", "tss"),
      labels = c("Chla (ug/L)",
                 "DOC (mg/L)",
                 "SDD (m)",
                 "SSC (mg/L)",
                 "TSS (mg/L)")
    )
  ) %>%
  group_by(param_units) %>%
  # Create factor based on quantile breaks
  mutate(q_values = cut(x = harmonized_value,
                        breaks = quantile(harmonized_value, quantile_perc),
                        include.lowest = TRUE, 
                        labels = quantile_labs)) %>%
  # Get mins, maxes per param * quantile
  group_by(param_units, q_values) %>%
  summarize(cut_min = min(harmonized_value, na.rm = TRUE),
            cut_max = max(harmonized_value, na.rm = TRUE),
            count = n()) %>%
  filter(!is.na(q_values)) 


ggplot() +
  geom_rect(data = matchup_quantiles,
            aes(xmin = cut_min, xmax = cut_max,
                ymin = 1, ymax = 10^6,
                fill = q_values),
            color = NA) +
  facet_wrap(vars(param_units), scales = "free") +
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
  geom_freqpoly(data = quantile_plot_data, position = "dodge",
                aes(x = harmonized_value, color = dataset)) +
  scale_fill_manual(values = quantile_colors, name = "Matchup quantiles") +
  scale_color_manual(values = c("red3", "black"), name = "Dataset") + 
  theme_bw() +
  theme(legend.position = "top", legend.direction = "horizontal", legend.box = "vertical") +
  ylab("Count") + 
  xlab("Depth or Concentration") +
  guides(fill = guide_legend(byrow = T))

rm(quantile_plot_data, siteSR_quantile_df)
gc()
```


Spectral response for each data quantile for each Landsat band:
```{r plot-bands} 
# Create some simple quality filters and a long dataset with bands arranged in
# a long format.
sr_long_median <- siteSR_matchups %>%
  mutate(ndvi = (med_Nir - med_Red) / (med_Nir + med_Red),
         type = ifelse(test = grepl("Lake", ResolvedMonitoringLocationTypeName),
                       yes = "Lake",
                       no = as.character(ResolvedMonitoringLocationTypeName)),
         type = factor(type)) %>%
  filter(ndvi < 0.5,
         med_Swir2 < 300,
         prop_clouds< .50,
         pCount_dswe1 > 9) %>%
  select(siteSR_id, ActivityStartDate, date, lat, lon,
         med_Blue, med_Green, med_Red, med_Nir, med_Swir1, med_Swir2,
         parameter, harmonized_value, harmonized_units, time_diff) %>%
  pivot_longer(names_to = "band", values_to = "refl",
               c(med_Blue, med_Green, med_Red, med_Nir, med_Swir1, med_Swir2)) %>%
  mutate(band = factor(band,
                       levels = c("med_Blue", "med_Green", "med_Red", "med_Nir",
                                  "med_Swir1", "med_Swir2"),
                       labels = c("blue","green","red","nir",
                                  "swir1","swir2"))) %>%
  filter(refl > 0,
         !is.na(refl)) %>%
  ungroup()

sr_long_same <- sr_long_median %>%
  # Previous filter to restrict to same date
  # filter(date == as.Date(ActivityStartDate)) %>%
  # Don't think was being used
  # mutate(tss_bin = cut(tss,
  #                    breaks=c(min(tss,na.rm=T),10,50,max(tss,na.rm=T)),
  #                    labels=c("< 10 mg/L","10-50 mg/L",">50 mg/L"))) %>%
  # mutate(tss_keep = tss) %>%
  # gather(key=parameter,value=value,-SiteID,-date_unity,
  #        -date,-band,-refl,-type,-tss.bin,-lat,-long,-tss.keep,-timediff) %>%
  ungroup() %>%
  group_by(parameter) %>%
  # Previous filter, but removes everything that's not SDD
  # filter(harmonized_value > 0,
  #        log10(harmonized_value) < mean(log10(harmonized_value)) + sd(log10(harmonized_value)) * 4 &
  #          log10(harmonized_value) > mean(log10(harmonized_value)) - sd(log10(harmonized_value)) * 4) %>%
  mutate(quintiles = cut(harmonized_value,
                         quantile(harmonized_value, quantile_perc),
                         labels = quantile_labs)) %>%
  group_by(parameter, quintiles) %>%
  mutate(n = n(),
         q_med = median(refl)) %>%
  ungroup()

# Plot data by quantile and band
sr_long_same %>%
  filter(!is.na(quintiles)) %>%
  mutate(quintiles = factor(quintiles, levels = quantile_labs)) %>%
  ggplot(., aes(x = band, y = refl, fill = quintiles)) +
  # ylim(0, 1500) +
  geom_boxplot(outlier.shape = NA, position = "dodge") +
  facet_wrap(vars(parameter)) +
  scale_fill_manual(values = quantile_colors, name = "") +
  theme(legend.position = "top", legend.direction = "horizontal") +
  ylab("Surface reflectance") +
  xlab("Band") +
  guides(fill = guide_legend(byrow = T))
```

```{r cleanup}
rm(sr_long_median, sr_long_same, matchup_quantiles, all_in_situ)
gc()
```


## Applying correction coefficients

Last, we'll apply the correction coefficients from lakeSR. They will also 
eventually be in the data pub, but for now, we can grab them from Google Drive.
Downloading is the first step:
```{r download-handoffs}
handoff_download_path <- "in/lakeSR_data/"

# Check for download directory, make if it does not exist
if (!dir.exists(handoff_download_path)) {
  dir.create(handoff_download_path, recursive = TRUE)
}

source(file = "src/retrieve_handoffs.R")
```

For now we filter for DSWE1, Deming or poly methods, and correcting only to
Landsat 7. Handoff model parameter get stacked, column names edited to combine
band and param names, and then pivoted back to a wider format.
```{r}
# First pivot for correction type
handoffs_wide <- handoffs %>%
  filter(dswe == "DSWE1a", method != "lm", sat_to == "LS7") %>%
  pivot_longer(
    cols = intercept:max_in_val,
    names_to = "coefs",
    values_to = "value"
  ) %>%
  mutate(new_column = paste(band, coefs, sep = "_")) %>%
  select(-band, -coefs) %>%
  pivot_wider(names_from = new_column, 
              values_from = value)
```

Now we'll join the Roy handoffs with the matchup dataset:
```{r apply-roy}
# Based on https://github.com/steeleb/regional-clarity-RS-model/ and
# https://github.com/rossyndicate/NW-CLP-RS/blob/main/f_apply_handoff_coefficients/src/apply_handoffs_to7.R
roy_corrected_temporary <- siteSR_matchups %>%
  # Standardize sat naming and coordinate with missions that are similar enough
  # to one another
  mutate(sat_harmonize = case_when(
    mission == "LT04" ~ "LS5",
    mission == "LT05" ~ "LS5",
    mission == "LE07" ~ "LS7",
    mission == "LC08" ~ "LS8",
    mission == "LC09" ~ "LS8",
    .default = NA_character_
  )) %>%
  left_join(
    x = .,
    y = handoffs_wide %>%
      filter(correction == "Roy") %>%
      select(-matches("B1|B2")),
    by = c("sat_harmonize" = "sat_corr")
  ) %>%
  mutate(
    red_corr7 = med_Red_intercept + med_Red_slope * med_Red,
    green_corr7 = med_Green_intercept + med_Green_slope * med_Green,
    blue_corr7 = med_Blue_intercept + med_Blue_slope * med_Blue,
    nir_corr7 = med_Nir_intercept + med_Nir_slope * med_Nir,
    surfacetemp_corr7 = med_SurfaceTemp_intercept + med_SurfaceTemp_slope * med_SurfaceTemp,
    
    # If mission was LandSat 7, then corrected vals should be NA because they
    # didn't need correction. Flag (NA in new col) if something weird is going on:
    # Blue
    blue_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(blue_corr7) ~ med_Blue,
      sat_harmonize != "LS7" & !is.na(blue_corr7) ~ blue_corr7,
      .default = NA_real_
    ),
    # Green
    green_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(green_corr7) ~ med_Green,
      sat_harmonize != "LS7" & !is.na(green_corr7) ~ green_corr7,
      .default = NA_real_
    ),
    # Red
    red_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(red_corr7) ~ med_Red,
      sat_harmonize != "LS7" & !is.na(red_corr7) ~ red_corr7,
      .default = NA_real_
    ),
    # NIR
    nir_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(nir_corr7) ~ med_Nir,
      sat_harmonize != "LS7" & !is.na(nir_corr7) ~ nir_corr7,
      .default = NA_real_
    ),
    # Surface temperature
    surfacetemp_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(surfacetemp_corr7) ~ med_SurfaceTemp,
      sat_harmonize != "LS7" & !is.na(surfacetemp_corr7) ~ surfacetemp_corr7,
      .default = NA_real_
    ),
    
    # Flag columns for when the values are out of the handoff input range
    # Blue
    flag_blue7 = ifelse(test = (med_Blue < med_Blue_max_in_val & 
                                  med_Blue > med_Blue_min_in_val) |
                          is.na(med_Blue_max_in_val),
                        yes = NA_character_, 
                        no = "extreme value"),
    # Green
    flag_green7 = ifelse(test = (med_Green < med_Green_max_in_val & 
                                   med_Green > med_Green_min_in_val) |
                           is.na(med_Green_max_in_val),
                         yes = NA_character_, 
                         no = "extreme value"),
    # Red
    flag_red7 = ifelse(test = (med_Red < med_Red_max_in_val & 
                                 med_Red > med_Red_min_in_val) |
                         is.na(med_Red_max_in_val),
                       yes = NA_character_, 
                       no = "extreme value"),
    # NIR 
    flag_nir7 = ifelse(test = (med_Nir < med_Nir_max_in_val & 
                                 med_Nir > med_Nir_min_in_val) |
                         is.na(med_Nir_max_in_val),
                       yes = NA_character_, 
                       no = "extreme value"),
    # Surface temperature
    flag_surfacetemp7 = ifelse(test = (med_SurfaceTemp < med_SurfaceTemp_max_in_val & 
                                         med_SurfaceTemp > med_SurfaceTemp_min_in_val) |
                                 is.na(med_SurfaceTemp_max_in_val),
                               yes = NA_character_, 
                               no = "extreme value")
  )

roy_corrected_temporary %>%
  filter(is.na(blue_corr7_new)) %>%
  nrow()
roy_corrected_temporary %>%
  filter(is.na(green_corr7_new)) %>%
  nrow()
roy_corrected_temporary %>%
  filter(is.na(red_corr7_new)) %>%
  nrow()
roy_corrected_temporary %>%
  filter(is.na(nir_corr7_new)) %>%
  nrow()
# ~13K rows, should originate from NA med_SurfaceTemp values
roy_corrected_temporary %>%
  filter(is.na(surfacetemp_corr7_new)) %>%
  nrow()

# No unexpected NAs in the new columns, so will simplify the final outputs
roy_corrected <- roy_corrected_temporary %>%
  mutate(
    blue_corr7 = blue_corr7_new,
    green_corr7 = green_corr7_new,
    red_corr7 = red_corr7_new,
    nir_corr7 = nir_corr7_new,
    surfacetemp_corr7 = surfacetemp_corr7_new
  ) %>%
  select(
    -c(med_Blue_intercept:med_SurfaceTemp_max_in_val),
    -contains("_new")
  )

# How many values per band are flagged for being out of the input range?
roy_corrected %>%
  summarise(
    across(contains("flag_"),
           ~sum(str_count(as.character(.),
                          regex("extreme value", ignore_case = TRUE)
           ), na.rm = TRUE
           )
    )
  ) %>%
  pivot_longer(cols = everything()) %>%
  arrange(desc(value))

rm(roy_corrected_temporary)
gc()
```

And then Gardner:
```{r apply-gardner}
# Based on https://github.com/steeleb/regional-clarity-RS-model/ and
# https://github.com/rossyndicate/NW-CLP-RS/blob/main/f_apply_handoff_coefficients/src/apply_handoffs_to7.R
gardner_corrected_temporary <- siteSR_matchups %>%
  # Coordinate sat naming with missions that are similar enough to one another
  mutate(sat_harmonize = case_when(
    mission == "LT04" ~ "LS5",
    mission == "LT05" ~ "LS5",
    mission == "LE07" ~ "LS7",
    mission == "LC08" ~ "LS8",
    mission == "LC09" ~ "LS8",
    .default = NA_character_
  )) %>%
  left_join(
    x = .,
    y = handoffs_wide %>%
      filter(correction == "Gardner") %>%
      select(-contains("slope")),
    by = c("sat_harmonize" = "sat_corr")
  ) %>%
  mutate(
    
    # Do the handoff calcs
    red_corr7 = med_Red_intercept + med_Red_B1 * med_Red + med_Red_B2 * med_Red^2,
    green_corr7 = med_Green_intercept + med_Green_B1 * med_Green + med_Green_B2 * med_Green^2,
    blue_corr7 = med_Blue_intercept + med_Blue_B1 * med_Blue + med_Blue_B2 * med_Blue^2,
    nir_corr7 = med_Nir_intercept + med_Nir_B1 * med_Nir + med_Nir_B2 * med_Nir^2,
    surfacetemp_corr7 = med_SurfaceTemp_intercept + med_SurfaceTemp_B1 * med_SurfaceTemp + med_SurfaceTemp_B2 * med_SurfaceTemp^2,
    
    # If mission was LandSat 7, then corrected vals should be NA because they
    # didn't need correction. Flag (NA) if something weird is going on:
    # Blue
    blue_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(blue_corr7) ~ med_Blue,
      sat_harmonize != "LS7" & !is.na(blue_corr7) ~ blue_corr7,
      .default = NA_real_
    ),
    # Green
    green_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(green_corr7) ~ med_Green,
      sat_harmonize != "LS7" & !is.na(green_corr7) ~ green_corr7,
      .default = NA_real_
    ),
    # Red
    red_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(red_corr7) ~ med_Red,
      sat_harmonize != "LS7" & !is.na(red_corr7) ~ red_corr7,
      .default = NA_real_
    ),
    # NIR
    nir_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(nir_corr7) ~ med_Nir,
      sat_harmonize != "LS7" & !is.na(nir_corr7) ~ nir_corr7,
      .default = NA_real_
    ),
    # Surface temperature
    surfacetemp_corr7_new = case_when(
      sat_harmonize == "LS7" & is.na(surfacetemp_corr7) ~ med_SurfaceTemp,
      sat_harmonize != "LS7" & !is.na(surfacetemp_corr7) ~ surfacetemp_corr7,
      .default = NA_real_
    ),
    
    # Flag columns for when the values are out of the handoff input range
    # Blue
    flag_blue7 = ifelse(test = (med_Blue < med_Blue_max_in_val & 
                                  med_Blue > med_Blue_min_in_val) |
                          is.na(med_Blue_max_in_val),
                        yes = NA_character_, 
                        no = "extreme value"),
    # Green
    flag_green7 = ifelse(test = (med_Green < med_Green_max_in_val & 
                                   med_Green > med_Green_min_in_val) |
                           is.na(med_Green_max_in_val),
                         yes = NA_character_, 
                         no = "extreme value"),
    # Red
    flag_red7 = ifelse(test = (med_Red < med_Red_max_in_val & 
                                 med_Red > med_Red_min_in_val) |
                         is.na(med_Red_max_in_val),
                       yes = NA_character_, 
                       no = "extreme value"),
    # NIR 
    flag_nir7 = ifelse(test = (med_Nir < med_Nir_max_in_val & 
                                 med_Nir > med_Nir_min_in_val) |
                         is.na(med_Nir_max_in_val),
                       yes = NA_character_, 
                       no = "extreme value"),
    # Surface temperature
    flag_surfacetemp7 = ifelse(test = (med_SurfaceTemp < med_SurfaceTemp_max_in_val & 
                                         med_SurfaceTemp > med_SurfaceTemp_min_in_val) |
                                 is.na(med_SurfaceTemp_max_in_val),
                               yes = NA_character_, 
                               no = "extreme value")
  )

gardner_corrected_temporary %>%
  filter(is.na(blue_corr7_new)) %>%
  nrow()
gardner_corrected_temporary %>%
  filter(is.na(green_corr7_new)) %>%
  nrow()
gardner_corrected_temporary %>%
  filter(is.na(red_corr7_new)) %>%
  nrow()
gardner_corrected_temporary %>%
  filter(is.na(nir_corr7_new)) %>%
  nrow()
# ~13K rows, should originate from NA med_SurfaceTemp values
gardner_corrected_temporary %>%
  filter(is.na(surfacetemp_corr7_new)) %>%
  nrow()

# No unexpected NAs in the new columns, so will simplify the final outputs
gardner_corrected <- gardner_corrected_temporary %>%
  mutate(
    blue_corr7 = blue_corr7_new,
    green_corr7 = green_corr7_new,
    red_corr7 = red_corr7_new,
    nir_corr7 = nir_corr7_new,
    surfacetemp_corr7 = surfacetemp_corr7_new
  ) %>%
  select(
    -c(med_Blue_intercept:med_SurfaceTemp_max_in_val),
    -contains("_new")
  )

# How many values per band are flagged for being out of the input range?
gardner_corrected %>%
  summarise(
    across(contains("flag_"),
           ~sum(str_count(as.character(.),
                          regex("extreme value", ignore_case = TRUE)
           ), na.rm = TRUE
           )
    )
  ) %>%
  pivot_longer(cols = everything()) %>%
  arrange(desc(value))

rm(gardner_corrected_temporary)
gc()

```

Export the final products:
```{r export}
# Where to export the matchup data
output_path <- "out/"

# Check for download directory, make if it does not exist
if (!dir.exists(output_path)) {
  dir.create(output_path)
}

# Export matchups
write_feather(x = roy_corrected,
              sink = file.path(output_path,
                               "siteSR_matchups_roy_correction.feather"))

write_feather(x = gardner_corrected,
              sink = file.path(output_path,
                               "siteSR_matchups_gardner_correction.feather"))

```